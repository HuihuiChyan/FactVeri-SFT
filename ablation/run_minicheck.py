import json
import os
import sys
import logging
from typing import List, Dict, Union
from scipy.stats import kendalltau

# from minicheck.minicheck import MiniCheck
from transformers import AutoModelForSequenceClassification

# --- é…ç½® ---
MODEL_NAME = 'Bespoke-MiniCheck-7B'
CACHE_DIR = '/workspace/HFModels'

def get_ranking_from_scores(scores: List[Union[int, float]]) -> List[int]:
    """
    æ ¹æ®åˆ†æ•°åˆ—è¡¨ç”Ÿæˆæ’åã€‚
    åˆ†æ•°è¶Šé«˜ï¼Œæ’åè¶Šé å‰ (reverse=True)ã€‚
    è¿”å›çš„æ˜¯ä¸€ä¸ª 0-based ç´¢å¼•åˆ—è¡¨ï¼Œè¡¨ç¤ºæ’åé¡ºåºã€‚
    
    ä¾‹å¦‚: scores = [0.8, 0.9, 0.5] (ç´¢å¼•0, 1, 2çš„åˆ†æ•°)
    è¿”å›: [1, 0, 2] 
    (è¡¨ç¤º: ç´¢å¼•1æ’ç¬¬ä¸€, ç´¢å¼•0æ’ç¬¬äºŒ, ç´¢å¼•2æ’ç¬¬ä¸‰)
    """
    indexed_scores = list(enumerate(scores))
    
    # *** å…³é”®: reverse=True ***
    # å› ä¸º raw_prob æ˜¯ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œè¶Šé«˜è¶Šå¥½ï¼Œæ‰€ä»¥æˆ‘ä»¬é™åºæ’åºã€‚
    sorted_by_score = sorted(indexed_scores, key=lambda item: item[1], reverse=True)
    
    ranking = [item[0] for item in sorted_by_score]
    return ranking


def evaluate_final_results_ranking(results: List[Dict]):
    """
    è®¡ç®—å¹¶æ‰“å°æ’åæ–¹æ¡ˆçš„è¯„ä¼°æŒ‡æ ‡ (P@1, Kendall's Tau)ã€‚
    (æ­¤å‡½æ•°æ¥è‡ªæ‚¨çš„ç¤ºä¾‹ï¼Œç”¨äºå¤„ç† 0-based æ’ååˆ—è¡¨)
    """
    kendall_tau_scores = []
    top_1_correct_count, valid_evaluation_count, invalid_predictions = 0, 0, 0
    total_items = len(results)
    
    if total_items == 0:
        logging.warning("æ²¡æœ‰æ”¶åˆ°ä»»ä½•ç»“æœç”¨äºè¯„ä¼°ã€‚")
        return None

    for item in results:
        true_ranking = item.get("verify_result")     # æœŸæœ›æ ¼å¼: [1, 0, 2]
        pred_ranking = item.get("predicted_ranking") # æœŸæœ›æ ¼å¼: [1, 0, 2]
        
        is_true_label_valid = isinstance(true_ranking, list) and true_ranking
        if not is_true_label_valid: 
            continue

        is_pred_valid = isinstance(pred_ranking, list) and pred_ranking
        if not is_pred_valid or len(true_ranking) != len(pred_ranking):
            invalid_predictions += 1
            continue

        valid_evaluation_count += 1
        num_answers = len(true_ranking)

        # æ£€æŸ¥ P@1 (æ’åç¬¬ä¸€çš„ç´¢å¼•æ˜¯å¦ç›¸åŒ)
        if true_ranking[0] == pred_ranking[0]:
            top_1_correct_count += 1

        # --- è®¡ç®— Kendall's Tau ---
        # å°† *æ’ååˆ—è¡¨* (å¦‚ [1, 0, 2]) è½¬æ¢ä¸º *é¡¹ç›®ç§©åˆ—è¡¨* (å¦‚ [1, 0, 2] -> [1, 0, 2])
        true_ranks = [0] * num_answers
        for rank, item_idx in enumerate(true_ranking):
            if 0 <= item_idx < num_answers:
                true_ranks[item_idx] = rank
            else:
                logging.warning(f"åœ¨ true_ranking ä¸­å‘ç°æ— æ•ˆç´¢å¼•: {item_idx}")

        pred_ranks = [0] * num_answers
        for rank, item_idx in enumerate(pred_ranking):
            if 0 <= item_idx < num_answers:
                pred_ranks[item_idx] = rank
            else:
                logging.warning(f"åœ¨ pred_ranking ä¸­å‘ç°æ— æ•ˆç´¢å¼•: {item_idx}")

        try:
            tau, _ = kendalltau(true_ranks, pred_ranks)
            kendall_tau_scores.append(tau)
        except ValueError as e:
            logging.warning(f"è®¡ç®— Kendall's Tau æ—¶å‡ºé”™: {e}ã€‚çœŸå®ç§©: {true_ranks}, é¢„æµ‹ç§©: {pred_ranks}")

    if valid_evaluation_count == 0:
        logging.error("è¯„ä¼°å¤±è´¥ã€‚æ²¡æœ‰æœ‰æ•ˆçš„é¡¹ç›®å¯ä¾›è¯„ä¼°ã€‚")
        return None

    # --- è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ ---
    precision_at_1 = top_1_correct_count / valid_evaluation_count if valid_evaluation_count else 0.0
    avg_kendall_tau = sum(kendall_tau_scores) / len(kendall_tau_scores) if kendall_tau_scores else 0.0
    invalid_ratio = invalid_predictions / total_items if total_items > 0 else 0.0

    metrics_dict = {
        "precision_at_1": round(precision_at_1, 4),
        "average_kendall_tau": round(avg_kendall_tau, 4),
        "invalid_prediction_ratio": round(invalid_ratio, 4),
        "valid_evaluation_count": valid_evaluation_count,
        "total_items_processed": total_items,
    }

    print("\n--- ğŸ“Š æ’åè¯„ä¼°ç»“æœ ---")
    for key, value in metrics_dict.items(): 
        print(f"{key.replace('_', ' ').title()}: {value}")
    print("----------------------------------\n")
    return metrics_dict


# -------------------------------------------------------------------
# 3. åˆå¹¶åçš„ä¸»å¤„ç†å‡½æ•°
# -------------------------------------------------------------------

def process_and_evaluate_file(filepath: str, scorer):
    """
    ä» .jsonl æ–‡ä»¶è¯»å–æ•°æ®ï¼Œä½¿ç”¨ MiniCheck è®¡ç®—åˆ†æ•°ç”Ÿæˆæ’åï¼Œå¹¶è°ƒç”¨è¯„ä¼°å‡½æ•°ã€‚
    """
    print(f"æ­£åœ¨å¼€å§‹åˆ†ææ–‡ä»¶: {filepath}...")
    
    results_for_evaluation: List[Dict] = [] # ç”¨äºå­˜å‚¨æ‰€æœ‰æ’åçš„åˆ—è¡¨
    total_lines = 0
    error_lines = 0
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                line_num = i + 1
                total_lines += 1
                
                try:
                    data = json.loads(line)

                    # 1. æå– 'answers' åˆ—è¡¨
                    if 'answers' not in data or not isinstance(data['answers'], list) or not data['answers']:
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'answers' é”®ç¼ºå¤±ã€ä¸æ˜¯åˆ—è¡¨æˆ–ä¸ºç©ºã€‚")
                        error_lines += 1
                        continue
                    
                    answers_data = data['answers']
                    
                    # 2. æå– 'verify_result' (1-based æ’å) åˆ—è¡¨
                    if 'verify_result' not in data or not isinstance(data['verify_result'], list):
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' é”®ç¼ºå¤±æˆ–ä¸æ˜¯åˆ—è¡¨ã€‚")
                        error_lines += 1
                        continue
                    
                    verified_1_based_ranking = data['verify_result'] # è¿™æ˜¯ 1-based æ’å, å¦‚ [2, 1, 3]

                    # 3. æå–ç”¨äº MiniCheck è®¡åˆ†çš„æ•°æ®
                    docs_list = []      # 'useful_facts'
                    claims_list = []    # 'answer'
                    
                    for item in answers_data:
                        useful_facts = item.get('useful_facts')
                        answer_text = item.get('answer')
                        if useful_facts is None or answer_text is None:
                            # å¦‚æœä»»ä¸€è¡Œä¸ºç©ºï¼Œåˆ™æ­¤è¡Œæ— æ³•è¯„ä¼°
                            raise ValueError(f"ç¬¬ {line_num} è¡Œçš„ 'answers' åˆ—è¡¨ä¸­ç¼ºå°‘ 'useful_facts' æˆ– 'answer'ã€‚")
                        docs_list.append(useful_facts)
                        claims_list.append(answer_text)

                    # 4. æ£€æŸ¥åˆ—è¡¨é•¿åº¦æ˜¯å¦ä¸€è‡´
                    if len(claims_list) != len(verified_1_based_ranking):
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'answers' åˆ—è¡¨ ({len(claims_list)}) ä¸ 'verify_result' ({len(verified_1_based_ranking)}) é•¿åº¦ä¸åŒ¹é…ã€‚")
                        error_lines += 1
                        continue

                    # 5. ç”Ÿæˆæ’å
                    
                    # (A) é¢„æµ‹æ’å: 
                    #    (i) è®¡ç®— MiniCheck åˆ†æ•° (raw_prob)
                    # print(f"æ­£åœ¨ä¸ºç¬¬ {line_num} è¡Œçš„ {len(claims_list)} ä¸ªç­”æ¡ˆè®¡ç®—å¾—åˆ†...")
                    # _, raw_prob_list, _, _ = scorer.score(docs=docs_list, claims=claims_list)
                    pairs = [(doc, claim) for doc, claim in zip(docs_list, claims_list)]
                    raw_prob_list = scorer.predict(pairs)
                    
                    #    (ii) ä»åˆ†æ•° -> 0-based æ’å
                    predicted_ranking_list = get_ranking_from_scores(raw_prob_list)
                    
                    # (B) çœŸå®æ’å: ä» 1-based æ’å -> 0-based æ’å
                    try:
                        # e.g., [2, 1, 3] -> [1, 0, 2]
                        verified_0_based_ranking = [idx - 1 for idx in verified_1_based_ranking]
                        
                        # éªŒè¯è½¬æ¢åçš„ç´¢å¼•
                        num_items = len(verified_0_based_ranking)
                        if not all(0 <= idx < num_items for idx in verified_0_based_ranking):
                            logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' åŒ…å«æ— æ•ˆçš„ 1-based ç´¢å¼• (ä¾‹å¦‚ 0 æˆ– å¤§äº {num_items})ã€‚")
                            error_lines += 1
                            continue
                        if len(set(verified_0_based_ranking)) != num_items:
                            logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' è½¬æ¢ååŒ…å«é‡å¤çš„ 0-based ç´¢å¼•ã€‚")
                            error_lines += 1
                            continue
                    
                    except TypeError:
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' åŒ…å«éæ•´æ•°é¡¹ã€‚")
                        error_lines += 1
                        continue
                    
                    # 6. å°†æ’ååˆ—è¡¨æ·»åŠ åˆ°æˆ‘ä»¬çš„ç»“æœé›†ä¸­
                    results_for_evaluation.append({
                        "verify_result": verified_0_based_ranking,   # æ ¼å¼: [1, 0, 2]
                        "predicted_ranking": predicted_ranking_list  # æ ¼å¼: [1, 0, 2]
                    })

                except json.JSONDecodeError:
                    logging.error(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: JSON è§£æé”™è¯¯ã€‚")
                    error_lines += 1
                except (KeyError, TypeError, AttributeError, ValueError) as e:
                    logging.error(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: å¤„ç†æ•°æ®æ—¶å‡ºé”™ - {e}")
                    error_lines += 1

    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ '{filepath}' æœªæ‰¾åˆ°ã€‚", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)

    # --- æ–‡ä»¶å¤„ç†æ‘˜è¦ ---
    print("\n--- ğŸ“ æ–‡ä»¶å¤„ç†æ‘˜è¦ ---")
    print(f"æ€»å…±æ£€æŸ¥è¡Œæ•°: {total_lines}")
    print(f"è·³è¿‡/é”™è¯¯è¡Œæ•°: {error_lines}")
    valid_lines = total_lines - error_lines
    print(f"æœ‰æ•ˆå‚ä¸è¯„ä¼°è¡Œæ•°: {valid_lines}")

    if valid_lines > 0:
        # 7. (å¾ªç¯ç»“æŸå) è°ƒç”¨è¯„ä¼°å‡½æ•°
        print("æ­£åœ¨è®¡ç®—æœ€ç»ˆæ’åç»Ÿè®¡æ•°æ®...")
        evaluate_final_results_ranking(results_for_evaluation)
    else:
        print("æ²¡æœ‰å¯ç”¨äºè¯„ä¼°çš„æœ‰æ•ˆæ•°æ®è¡Œã€‚")


# --- è„šæœ¬ä¸»å…¥å£ ---
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—è®°å½•
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s') 
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python rank_and_evaluate.py <your_file.jsonl>")
        print("ç¤ºä¾‹: python rank_and_evaluate.py data.jsonl")
        sys.exit(1)
        
    file_path = sys.argv[1]
    
    # 1. åˆå§‹åŒ– Scorer (å…¨å±€ä¸€æ¬¡)
    # scorer = MiniCheck(model_name=MODEL_NAME, enable_prefix_caching=False, cache_dir=CACHE_DIR)
    # Step 1: Load the model
    scorer = AutoModelForSequenceClassification.from_pretrained(
    '/workspace/HFModels/hallucination_evaluation_model', trust_remote_code=True)
    
    # 2. å¤„ç†æ–‡ä»¶å¹¶è¯„ä¼°
    process_and_evaluate_file(file_path, scorer)