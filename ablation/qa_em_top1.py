# Copyright 2024 Bytedance Ltd. and/or its affiliates
# ( ... Apache License 2.0 ... )

import re
import string
import random
import json
import sys
from collections import Counter  # å¯¼å…¥ Counterï¼Œç”¨äºè®¡ç®—F1

# --- æ ¸å¿ƒå‡½æ•° (normalize_answer, em_check) (ä¿ç•™) ---

def normalize_answer(s):
    """å°å†™ã€ç§»é™¤æ ‡ç‚¹ã€æ–‡ç« å’Œå¤šä½™ç©ºæ ¼"""
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def em_check(prediction, golden_answers):
    """æ£€æŸ¥é¢„æµ‹æ˜¯å¦ä¸ä»»ä½•ä¸€ä¸ªæ ‡å‡†ç­”æ¡ˆå®Œå…¨åŒ¹é… (ç»è¿‡æ ‡å‡†åŒ–å¤„ç†)"""
    if isinstance(golden_answers, str):
        golden_answers = [golden_answers]
    
    if prediction is None:
        prediction = ""
        
    normalized_prediction = normalize_answer(prediction)
    score = 0
    for golden_answer in golden_answers:
        if golden_answer is None:
            golden_answer = ""
            
        golden_answer = normalize_answer(golden_answer)
        if golden_answer == normalized_prediction:
            score = 1
            break
    return score

# --- æ–°å¢ï¼šF1 Score è®¡ç®—å‡½æ•° ---

def compute_f1(prediction, ground_truth):
    """è®¡ç®—å•ä¸ªé¢„æµ‹å’Œå•ä¸ªæ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„F1 score"""
    # æ ‡å‡†åŒ–
    prediction_normalized = normalize_answer(prediction)
    ground_truth_normalized = normalize_answer(ground_truth)
    
    # åˆ†è¯
    prediction_tokens = prediction_normalized.split()
    ground_truth_tokens = ground_truth_normalized.split()
    
    # å¤„ç†ç©ºå­—ç¬¦ä¸²çš„è¾¹ç¼˜æƒ…å†µ
    if not prediction_tokens and not ground_truth_tokens:
        return 1.0  # ä¸¤ä¸ªéƒ½æ˜¯ç©ºçš„ï¼Œç®—å®Œå…¨åŒ¹é…
    if not prediction_tokens or not ground_truth_tokens:
        return 0.0  # ä¸€ä¸ªæ˜¯ç©ºçš„ï¼Œä¸€ä¸ªæ˜¯ç©ºçš„ï¼ŒF1ä¸º0

    # ä½¿ç”¨ Counter è®¡ç®—äº¤é›†
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    
    if num_same == 0:
        return 0.0

    # è®¡ç®— Precision, Recall, F1
    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return f1

def f1_check(prediction, golden_answers):
    """
    è®¡ç®—é¢„æµ‹ä¸æ‰€æœ‰æ ‡å‡†ç­”æ¡ˆçš„F1 scoreï¼Œå¹¶è¿”å›æœ€é«˜åˆ†
    (QAè¯„æµ‹æ ‡å‡†ï¼šmax F1 over all ground truths)
    """
    if isinstance(golden_answers, str):
        golden_answers = [golden_answers]
    
    if prediction is None:
        prediction = ""

    # å¦‚æœ golden_answers åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ¯”è¾ƒ
    if not golden_answers:
        return 0.0 # æˆ–è€…æ ¹æ®éœ€è¦æŠ›å‡ºé”™è¯¯

    # è®¡ç®—ä¸æ¯ä¸€ä¸ª golden answer çš„F1ï¼Œå–æœ€å¤§å€¼
    max_f1 = 0.0
    for golden_answer in golden_answers:
        if golden_answer is None:
            golden_answer = ""
        
        f1 = compute_f1(prediction, golden_answer)
        if f1 > max_f1:
            max_f1 = f1
            
    return max_f1

# --- ä¿ç•™çš„å…¶ä»–å‡½æ•° (subem_check, extract_solution, etc.) ---
# (è¿™éƒ¨åˆ†åœ¨æœ¬æ¬¡è®¡ç®—ä¸­ä¸ä¼šè¢«è°ƒç”¨)
def subem_check(prediction, golden_answers):
    # ... (ä»£ç ä¸ä½ æä¾›çš„ä¸€è‡´, çœç•¥) ...
    pass

def extract_solution(solution_str):
    # ... (ä»£ç ä¸ä½ æä¾›çš„ä¸€è‡´, çœç•¥) ...
    pass

def compute_score_em(solution_str, ground_truth, method='strict', format_score=0., score=1.):
    # ... (ä»£ç ä¸ä½ æä¾›çš„ä¸€è‡´, çœç•¥) ...
    pass

def compute_score_subem(solution_str, ground_truth, method='strict', format_score=0., score=1.):
    # ... (ä»£ç ä¸ä½ æä¾›çš„ä¸€è‡´, çœç•¥) ...
    pass

# --- ä¿®æ”¹ï¼šç”¨äºè®¡ç®—JSONLæ–‡ä»¶çš„å¹³å‡EMå’ŒF1å¾—åˆ† ---

def calculate_average_metrics_from_jsonl(file_path):
    """
    è¯»å–æŒ‡å®šçš„JSONLæ–‡ä»¶ï¼Œè®¡ç®—æ¯è¡Œ 'answers' åˆ—è¡¨ä¸­ 'score' æœ€é«˜çš„
    'answer' ç›¸å¯¹äº 'reference' çš„ EM å’Œ F1 å¾—åˆ†ï¼Œå¹¶è¿”å›å¹³å‡åˆ†ã€‚
    """
    all_em_scores = []
    all_f1_scores = []
    total_lines = 0
    valid_lines = 0
    
    print(f"Processing file: {file_path} ...")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                total_lines += 1
                try:
                    data = json.loads(line.strip())
                    
                    # 1. è·å–æ ‡å‡†ç­”æ¡ˆ
                    golden_answers = data.get('reference')
                    
                    # 2. è·å– 'answers' åˆ—è¡¨
                    answers_list = data.get('answers')
                    
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦å®Œæ•´
                    if golden_answers is None or not answers_list:
                        print(f"Skipping line {total_lines}: Missing 'reference' or 'answers'.")
                        continue
                        
                    # --- ä¿®æ”¹ç‚¹ å¼€å§‹ ---
                    # 3. æ‰¾å‡º 'answers' åˆ—è¡¨ä¸­ 'score' æœ€é«˜çš„ answer
                    #    (å¦‚æœ 'score' é”®ä¸å­˜åœ¨æˆ– 'answers_list' ä¸ºç©º, 
                    #     å¤–å±‚çš„ try...except (KeyError, ValueError) ä¼šæ•è·å®ƒ)
                    
                    best_answer_obj = max(answers_list, key=lambda item: item['factuality_score'])
                    
                    # 4. è·å– 'score' æœ€é«˜çš„ 'answer'
                    prediction = best_answer_obj.get('answer') # ä½¿ç”¨ .get() ä¿æŒå¥å£®æ€§
                    # --- ä¿®æ”¹ç‚¹ ç»“æŸ ---

                    # 5. è®¡ç®—EMå’ŒF1å¾—åˆ†
                    score_em = em_check(prediction, golden_answers)
                    score_f1 = f1_check(prediction, golden_answers)
                    
                    all_em_scores.append(score_em)
                    all_f1_scores.append(score_f1)
                    
                    valid_lines += 1
                    
                except json.JSONDecodeError:
                    print(f"Skipping line {total_lines}: Invalid JSON format.")
                except (IndexError, TypeError, KeyError, ValueError) as e:
                    # ValueError å¯èƒ½ä¼šåœ¨ 'answers_list' ä¸ºç©ºæ—¶è¢« max() è§¦å‘ (è™½ç„¶å‰é¢å·²æ£€æŸ¥)
                    # KeyError å¯èƒ½ä¼šåœ¨ 'score' é”®ä¸å­˜åœ¨æ—¶è¢« lambda è§¦å‘
                    print(f"Skipping line {total_lines}: Data structure error ({e}).")

    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return

    # --- è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆç»“æœ ---
    if not all_em_scores: # æ£€æŸ¥ä»»ä¸€åˆ—è¡¨å‡å¯
        print("No valid data found to calculate score.")
    else:
        average_em = (sum(all_em_scores) / len(all_em_scores)) * 100 # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
        average_f1 = (sum(all_f1_scores) / len(all_f1_scores)) * 100 # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
        
        print("\n--- ğŸ“Š Results ---")
        print(f"Total lines read:        {total_lines}")
        print(f"Valid lines processed:   {valid_lines}")
        print(f"EM correct (count):      {sum(all_em_scores)}")
        print(f"Average EM Score:        {average_em:.2f}%")
        print(f"Average F1 Score:        {average_f1:.2f}%")


# --- ä¸»æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python compute_scores.py <path_to_your_jsonl_file>")
        sys.exit(1)
        
    JSONL_FILE_PATH = sys.argv[1]
    
    # è¿è¡Œè®¡ç®—
    calculate_average_metrics_from_jsonl(JSONL_FILE_PATH)