import json
import sys
import logging
from typing import List, Dict, Union
from scipy.stats import kendalltau

# è¿™ä¸ªå‡½æ•°æ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ä»¬ä¿ç•™å®ƒï¼Œç”¨äºå¤„ç† 'predicted_scoring'
def get_ranking_from_scores(scores: List[Union[int, float]]) -> List[int]:
    """
    æ ¹æ®åˆ†æ•°åˆ—è¡¨ç”Ÿæˆæ’åã€‚
    åˆ†æ•°è¶Šä½ï¼Œæ’åè¶Šé å‰ï¼ˆå‡åºæ’åºï¼‰ã€‚
    è¿”å›çš„æ˜¯ä¸€ä¸ªç´¢å¼•åˆ—è¡¨ï¼Œè¡¨ç¤ºæ’åé¡ºåºã€‚
    
    ä¾‹å¦‚: scores = [1.5, 3.1, 2.0]  (ç´¢å¼•0, 1, 2çš„åˆ†æ•°)
    è¿”å›: [0, 2, 1] 
    (è¡¨ç¤º: ç´¢å¼•0æ’ç¬¬ä¸€, ç´¢å¼•2æ’ç¬¬äºŒ, ç´¢å¼•1æ’ç¬¬ä¸‰)
    """
    
    # å°†åˆ†æ•°ä¸å®ƒä»¬çš„åŸå§‹ç´¢å¼•é…å¯¹ (index, score)
    indexed_scores = list(enumerate(scores))
    
    # æ ¹æ®åˆ†æ•°ï¼ˆå…ƒç»„çš„ç¬¬äºŒä¸ªå…ƒç´  item[1]ï¼‰è¿›è¡Œå‡åºæ’åº
    sorted_by_score = sorted(indexed_scores, key=lambda item: item[1], reverse=True)
    
    # æå–æ’åºåçš„åŸå§‹ç´¢å¼• (item[0])ï¼Œè¿™å°±æ˜¯æ’ååˆ—è¡¨
    ranking = [item[0] for item in sorted_by_score]
    return ranking


# è¿™ä¸ªè¯„ä¼°å‡½æ•°ä¹Ÿæ˜¯æ­£ç¡®çš„ï¼Œå®ƒæœŸæœ›çš„è¾“å…¥æ˜¯ 0-based æ’ååˆ—è¡¨
# (ä¾‹å¦‚ [0, 2, 1])
def evaluate_final_results_ranking(results: List[Dict]):
    """
    è®¡ç®—å¹¶æ‰“å°æ’åæ–¹æ¡ˆçš„è¯„ä¼°æŒ‡æ ‡ã€‚
    çœŸå®å€¼æ˜¯ä¸€ä¸ªæ’ååˆ—è¡¨ã€‚ä½¿ç”¨çš„æŒ‡æ ‡æ˜¯ P@1 å’Œ Kendall's Tauã€‚
    (è¿™æ˜¯æ‚¨æä¾›çš„å‡½æ•°ï¼Œå·²é›†æˆäº†0-basedç´¢å¼•ä¿®å¤)
    """
    kendall_tau_scores = []
    top_1_correct_count, valid_evaluation_count, invalid_predictions = 0, 0, 0
    total_items = len(results)
    
    if total_items == 0:
        logging.warning("æ²¡æœ‰æ”¶åˆ°ä»»ä½•ç»“æœç”¨äºè¯„ä¼°ã€‚")
        return None

    for item in results:
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æœŸæœ›çš„ "verify_result" å’Œ "predicted_ranking" 
        # æ˜¯ *0-based æ’ååˆ—è¡¨* (å¦‚ [0, 2, 1])
        true_ranking = item.get("verify_result")
        pred_ranking = item.get("predicted_ranking")
        
        is_true_label_valid = isinstance(true_ranking, list) and true_ranking
        if not is_true_label_valid: 
            continue

        is_pred_valid = isinstance(pred_ranking, list) and pred_ranking
        if not is_pred_valid or len(true_ranking) != len(pred_ranking):
            invalid_predictions += 1
            continue

        valid_evaluation_count += 1
        num_answers = len(true_ranking)

        # æ£€æŸ¥ P@1 (æ’åç¬¬ä¸€çš„ç´¢å¼•æ˜¯å¦ç›¸åŒ)
        if true_ranking[0] == pred_ranking[0]:
            top_1_correct_count += 1

        # --- è®¡ç®— Kendall's Tau ---
        # æˆ‘ä»¬éœ€è¦å°† *æ’ååˆ—è¡¨* (å¦‚ [0, 2, 1]) è½¬æ¢ä¸º *é¡¹ç›®ç§©åˆ—è¡¨* (å¦‚ [0, 2, 1])
        # æ’ååˆ—è¡¨: [idx_at_rank_0, idx_at_rank_1, idx_at_rank_2]
        # é¡¹ç›®ç§©åˆ—è¡¨: [rank_of_idx_0,   rank_of_idx_1,   rank_of_idx_2]

        true_ranks = [0] * num_answers
        for rank, item_idx in enumerate(true_ranking):
            if 0 <= item_idx < num_answers:
                true_ranks[item_idx] = rank
            else:
                logging.warning(f"åœ¨ true_ranking ä¸­å‘ç°æ— æ•ˆç´¢å¼•: {item_idx}")

        pred_ranks = [0] * num_answers
        for rank, item_idx in enumerate(pred_ranking):
            if 0 <= item_idx < num_answers:
                pred_ranks[item_idx] = rank
            else:
                logging.warning(f"åœ¨ pred_ranking ä¸­å‘ç°æ— æ•ˆç´¢å¼•: {item_idx}")


        tau, _ = kendalltau(true_ranks, pred_ranks)
        kendall_tau_scores.append(tau)

    if valid_evaluation_count == 0:
        logging.error("è¯„ä¼°å¤±è´¥ã€‚æ²¡æœ‰æœ‰æ•ˆçš„é¡¹ç›®å¯ä¾›è¯„ä¼°ã€‚")
        return None

    # --- è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ ---
    precision_at_1 = top_1_correct_count / valid_evaluation_count if valid_evaluation_count else 0.0
    avg_kendall_tau = sum(kendall_tau_scores) / len(kendall_tau_scores) if kendall_tau_scores else 0.0
    invalid_ratio = invalid_predictions / total_items if total_items > 0 else 0.0

    metrics_dict = {
        "precision_at_1": round(precision_at_1, 4),
        "average_kendall_tau": round(avg_kendall_tau, 4),
        "invalid_prediction_ratio": round(invalid_ratio, 4),
        "valid_evaluation_count": valid_evaluation_count,
        "total_items_processed": total_items,
    }

    print("\n--- ğŸ“Š æ’åè¯„ä¼°ç»“æœ ---")
    for key, value in metrics_dict.items(): 
        print(f"{key.replace('_', ' ').title()}: {value}")
    print("----------------------------------\n")
    return metrics_dict


def analyze_rankings_from_file(filepath: str):
    """
    ä» .jsonl æ–‡ä»¶è¯»å–æ•°æ®ï¼Œç”Ÿæˆæ’åï¼Œå¹¶è°ƒç”¨è¯„ä¼°å‡½æ•°ã€‚
    """
    print(f"æ­£åœ¨å¼€å§‹åˆ†ææ–‡ä»¶: {filepath}...")
    
    results_for_evaluation: List[Dict] = [] # ç”¨äºå­˜å‚¨æ‰€æœ‰æ’åçš„åˆ—è¡¨
    total_lines = 0
    error_lines = 0
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                line_num = i + 1
                total_lines += 1
                
                try:
                    data = json.loads(line)

                    # 1. æå– 'predicted_scoring' åˆ—è¡¨
                    if 'answers' not in data or not isinstance(data['answers'], list) or not data['answers']:
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'answers' é”®ç¼ºå¤±ã€ä¸æ˜¯åˆ—è¡¨æˆ–ä¸ºç©ºã€‚")
                        error_lines += 1
                        continue
                    
                    predicted_scores = [answer.get('predicted_scoring') for answer in data['answers']]
                    
                    if None in predicted_scores:
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'answers' åˆ—è¡¨ä¸­ç¼ºå°‘ 'predicted_scoring' é”®ã€‚")
                        error_lines += 1
                        continue

                    # 2. æå– 'verify_result' (1-based æ’å) åˆ—è¡¨
                    #    æ ¹æ®ä½ çš„æè¿°, 'verify_result' æ˜¯ [1, 3, 2] è¿™æ ·çš„ 1-based æ’ååˆ—è¡¨
                    if 'verify_result' not in data or not isinstance(data['verify_result'], list):
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' é”®ç¼ºå¤±æˆ–ä¸æ˜¯åˆ—è¡¨ã€‚")
                        error_lines += 1
                        continue
                        
                    verified_1_based_ranking = data['verify_result'] # è¿™æ˜¯ 1-based æ’å, å¦‚ [1, 3, 2]

                    # 3. æ£€æŸ¥åˆ—è¡¨é•¿åº¦æ˜¯å¦ä¸€è‡´
                    if len(predicted_scores) != len(verified_1_based_ranking):
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: åˆ†æ•°/æ’ååˆ—è¡¨é•¿åº¦ä¸åŒ¹é…ã€‚")
                        error_lines += 1
                        continue

                    # 4. ç”Ÿæˆæ’å
                    
                    # (A) é¢„æµ‹æ’å: ä»åˆ†æ•° -> 0-based æ’å (è¿™éƒ¨åˆ†æ˜¯æ­£ç¡®çš„)
                    predicted_ranking_list = get_ranking_from_scores(predicted_scores)
                    
                    # (B) çœŸå®æ’å: ä» 1-based æ’å -> 0-based æ’å (*** è¿™æ˜¯ä¿®æ”¹çš„éƒ¨åˆ† ***)
                    try:
                        # e.g., [1, 3, 2] -> [0, 2, 1]
                        verified_ranking_list = [idx - 1 for idx in verified_1_based_ranking]
                        
                        # (å¯é€‰ä½†æ¨è) æ£€æŸ¥è½¬æ¢åçš„ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                        num_items = len(verified_ranking_list)
                        if not all(0 <= idx < num_items for idx in verified_ranking_list):
                             logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' åŒ…å«æ— æ•ˆçš„ 1-based ç´¢å¼• (ä¾‹å¦‚ 0 æˆ– å¤§äº {num_items})ã€‚")
                             error_lines += 1
                             continue
                        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ç´¢å¼•ï¼Œè¿™åœ¨æ’åä¸­æ˜¯ä¸å…è®¸çš„
                        if len(set(verified_ranking_list)) != num_items:
                             logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' è½¬æ¢ååŒ…å«é‡å¤çš„ 0-based ç´¢å¼•ã€‚")
                             error_lines += 1
                             continue

                    except TypeError:
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' åŒ…å«éæ•´æ•°é¡¹ï¼Œæ— æ³•è½¬æ¢ä¸º 0-based ç´¢å¼•ã€‚")
                        error_lines += 1
                        continue
                    
                    # 5. å°†æ’ååˆ—è¡¨æ·»åŠ åˆ°æˆ‘ä»¬çš„ç»“æœé›†ä¸­
                    results_for_evaluation.append({
                        "verify_result": verified_ranking_list,      # æ ¼å¼: [0, 2, 1]
                        "predicted_ranking": predicted_ranking_list  # æ ¼å¼: [0, 1, 2]
                    })

                except json.JSONDecodeError:
                    logging.error(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: JSON è§£æé”™è¯¯ã€‚")
                    error_lines += 1
                except (KeyError, TypeError, AttributeError) as e:
                    logging.error(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: å¤„ç†æ•°æ®æ—¶å‡ºé”™ - {e}")
                    error_lines += 1

        # --- æ–‡ä»¶å¤„ç†æ‘˜è¦ ---
        print("\n--- ğŸ“ æ–‡ä»¶å¤„ç†æ‘˜è¦ ---")
        print(f"æ€»å…±æ£€æŸ¥è¡Œæ•°: {total_lines}")
        print(f"è·³è¿‡/é”™è¯¯è¡Œæ•°: {error_lines}")
        valid_lines = total_lines - error_lines
        print(f"æœ‰æ•ˆå‚ä¸è¯„ä¼°è¡Œæ•°: {valid_lines}")

        if valid_lines > 0:
            # 6. (å¾ªç¯ç»“æŸå) è°ƒç”¨è¯„ä¼°å‡½æ•°
            print("æ­£åœ¨è®¡ç®—æ’åç»Ÿè®¡æ•°æ®...")
            evaluate_final_results_ranking(results_for_evaluation)
        else:
            print("æ²¡æœ‰å¯ç”¨äºè¯„ä¼°çš„æœ‰æ•ˆæ•°æ®è¡Œã€‚")

    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ '{filepath}' æœªæ‰¾åˆ°ã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

# --- è„šæœ¬ä¸»å…¥å£ ---
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—è®°å½•
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s') 
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python evaluate_rankings.py <your_file.jsonl>")
        sys.exit(1)
        
    file_path = sys.argv[1]
    
    analyze_rankings_from_file(file_path)