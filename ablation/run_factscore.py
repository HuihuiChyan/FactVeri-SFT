import json
import os
import sys
import logging
from typing import List, Dict, Union
from scipy.stats import kendalltau

# --- é…ç½® ---
# !! é‡è¦: è¯·ä¿®æ”¹ä¸ºæ‚¨å­˜æ”¾ 1.json, 2.json, 3.json æ–‡ä»¶çš„ç›®å½•
# å‡è®¾: <SCORE_FILES_DIR>/1.json, <SCORE_FILES_DIR>/2.json, ...
SCORE_FILES_DIR = '.' 

# (è¿™éƒ¨åˆ†é…ç½®é€šå¸¸ä¸éœ€è¦ä¿®æ”¹)
# é”® 0 å¯¹åº” ä½ç½®0 (ans_index=0)
SCORE_FILENAMES = {
    0: '/workspace/FactVeri-SFT/corpora/factscore_veri/ChatGPT_select_factscore_output.json',
    1: '/workspace/FactVeri-SFT/corpora/factscore_veri/InstructGPT_select_factscore_output.json',
    2: '/workspace/FactVeri-SFT/corpora/factscore_veri/PerplexityAI_select_factscore_output.json',
}
SCORES_KEY = 'scores' # 1.json ç­‰æ–‡ä»¶ä¸­çš„åˆ†æ•°åˆ—è¡¨çš„é”®å
# --- æ›´æ”¹ç»“æŸ ---


def get_ranking_from_scores(scores: List[Union[int, float]]) -> List[int]:
    """
    æ ¹æ®åˆ†æ•°åˆ—è¡¨ç”Ÿæˆæ’åã€‚
    åˆ†æ•°è¶Šé«˜ï¼Œæ’åè¶Šé å‰ (reverse=True)ã€‚
    """
    indexed_scores = list(enumerate(scores))
    sorted_by_score = sorted(indexed_scores, key=lambda item: item[1], reverse=True)
    ranking = [item[0] for item in sorted_by_score]
    return ranking


def evaluate_final_results_ranking(results: List[Dict]):
    """
    è®¡ç®—å¹¶æ‰“å°æ’åæ–¹æ¡ˆçš„è¯„ä¼°æŒ‡æ ‡ (P@1, Kendall's Tau)ã€‚
    (æ­¤å‡½æ•°æ¥è‡ªæ‚¨çš„ç¤ºä¾‹ï¼Œç”¨äºå¤„ç† 0-based æ’ååˆ—è¡¨)
    """
    kendall_tau_scores = []
    top_1_correct_count, valid_evaluation_count, invalid_predictions = 0, 0, 0
    total_items = len(results)
    
    if total_items == 0:
        logging.warning("æ²¡æœ‰æ”¶åˆ°ä»»ä½•ç»“æœç”¨äºè¯„ä¼°ã€‚")
        return None

    for item in results:
        true_ranking = item.get("verify_result")      # æœŸæœ›æ ¼å¼: [1, 0, 2]
        pred_ranking = item.get("predicted_ranking") # æœŸæœ›æ ¼å¼: [1, 0, 2]
        
        is_true_label_valid = isinstance(true_ranking, list) and true_ranking
        if not is_true_label_valid: 
            continue

        is_pred_valid = isinstance(pred_ranking, list) and pred_ranking
        if not is_pred_valid or len(true_ranking) != len(pred_ranking):
            invalid_predictions += 1
            continue

        valid_evaluation_count += 1
        num_answers = len(true_ranking)

        # æ£€æŸ¥ P@1 (æ’åç¬¬ä¸€çš„ç´¢å¼•æ˜¯å¦ç›¸åŒ)
        if true_ranking[0] == pred_ranking[0]:
            top_1_correct_count += 1

        # --- è®¡ç®— Kendall's Tau ---
        true_ranks = [0] * num_answers
        for rank, item_idx in enumerate(true_ranking):
            if 0 <= item_idx < num_answers:
                true_ranks[item_idx] = rank
            else:
                logging.warning(f"åœ¨ true_ranking ä¸­å‘ç°æ— æ•ˆç´¢å¼•: {item_idx}")

        pred_ranks = [0] * num_answers
        for rank, item_idx in enumerate(pred_ranking):
            if 0 <= item_idx < num_answers:
                pred_ranks[item_idx] = rank
            else:
                logging.warning(f"åœ¨ pred_ranking ä¸­å‘ç°æ— æ•ˆç´¢å¼•: {item_idx}")

        try:
            tau, _ = kendalltau(true_ranks, pred_ranks)
            kendall_tau_scores.append(tau)
        except ValueError as e:
            logging.warning(f"è®¡ç®— Kendall's Tau æ—¶å‡ºé”™: {e}ã€‚çœŸå®ç§©: {true_ranks}, é¢„æµ‹ç§©: {pred_ranks}")

    if valid_evaluation_count == 0:
        logging.error("è¯„ä¼°å¤±è´¥ã€‚æ²¡æœ‰æœ‰æ•ˆçš„é¡¹ç›®å¯ä¾›è¯„ä¼°ã€‚")
        return None

    # --- è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ ---
    precision_at_1 = top_1_correct_count / valid_evaluation_count if valid_evaluation_count else 0.0
    avg_kendall_tau = sum(kendall_tau_scores) / len(kendall_tau_scores) if kendall_tau_scores else 0.0
    invalid_ratio = invalid_predictions / total_items if total_items > 0 else 0.0

    metrics_dict = {
        "precision_at_1": round(precision_at_1, 4),
        "average_kendall_tau": round(avg_kendall_tau, 4),
        "invalid_prediction_ratio": round(invalid_ratio, 4),
        "valid_evaluation_count": valid_evaluation_count,
        "total_items_processed": total_items,
    }

    print("\n--- ğŸ“Š æ’åè¯„ä¼°ç»“æœ ---")
    for key, value in metrics_dict.items(): 
        print(f"{key.replace('_', ' ').title()}: {value}")
    print("----------------------------------\n")
    return metrics_dict


# -------------------------------------------------------------------
# (æ›´æ”¹) æ–°çš„è¾…åŠ©å‡½æ•°ï¼šç”¨äºé¢„åŠ è½½æ‰€æœ‰åˆ†æ•°
# -------------------------------------------------------------------

def load_all_scores(base_dir: str) -> Dict[int, List[float]]:
    """
    ä» 1.json, 2.json, 3.json åŠ è½½æ‰€æœ‰åˆ†æ•°åˆ—è¡¨ã€‚
    è¿”å›ä¸€ä¸ªå­—å…¸: {0: [scores_list_pos0], 1: [scores_list_pos1], 2: [scores_list_pos2]}
    """
    logging.info("æ­£åœ¨é¢„åŠ è½½æ‰€æœ‰åˆ†æ•°æ–‡ä»¶...")
    all_scores = {}
    expected_length = -1
    
    # ç¡®ä¿æŒ‰é¡ºåº 0, 1, 2 åŠ è½½
    for position in sorted(SCORE_FILENAMES.keys()):
        filename = SCORE_FILENAMES[position]
        filepath = filename
        
        # try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if SCORES_KEY not in data:
            logging.error(f"é”™è¯¯: åˆ†æ•°æ–‡ä»¶ '{filepath}' ä¸­ç¼ºå°‘ '{SCORES_KEY}' é”®ã€‚")
            sys.exit(1)
        
        scores_list = data[SCORES_KEY]
        
        if not isinstance(scores_list, list):
            logging.error(f"é”™è¯¯: '{filepath}' ä¸­çš„ '{SCORES_KEY}' ä¸æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚")
            sys.exit(1)

        all_scores[position] = scores_list
        current_length = len(scores_list)
        
        # éªŒè¯æ‰€æœ‰åˆ†æ•°åˆ—è¡¨çš„é•¿åº¦æ˜¯å¦ä¸€è‡´
        if expected_length == -1:
            expected_length = current_length
        elif expected_length != current_length:
            logging.error(f"é”™è¯¯: åˆ†æ•°æ–‡ä»¶ '{filename}' çš„é•¿åº¦ ({current_length}) ä¸ '1.json' çš„é•¿åº¦ ({expected_length}) ä¸åŒ¹é…ã€‚")
            logging.error("é”™è¯¯: 1.json, 2.json, 3.json ä¸­çš„ 'scores' åˆ—è¡¨é•¿åº¦å¿…é¡»å®Œå…¨ä¸€è‡´ã€‚")
            sys.exit(1)

        # except FileNotFoundError:
        #     logging.error(f"é”™è¯¯: æ‰¾ä¸åˆ°å¿…è¦çš„åˆ†æ•°æ–‡ä»¶: {filepath}")
        #     sys.exit(1)
        # except json.JSONDecodeError:
        #     logging.error(f"é”™è¯¯: è§£æåˆ†æ•°æ–‡ä»¶ '{filepath}' (JSON) æ—¶å‡ºé”™ã€‚")
        #     sys.exit(1)
    
    if len(all_scores) != 3:
        logging.error("é”™è¯¯: æœªèƒ½æˆåŠŸåŠ è½½æ‰€æœ‰ 3 ä¸ªåˆ†æ•°æ–‡ä»¶ã€‚")
        sys.exit(1)
        
    logging.info(f"âœ… æˆåŠŸåŠ è½½ 3 ä¸ªåˆ†æ•°åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ—è¡¨åŒ…å« {expected_length} ä¸ªåˆ†æ•°ã€‚")
    return all_scores

# -------------------------------------------------------------------
# 3. åˆå¹¶åçš„ä¸»å¤„ç†å‡½æ•°
# -------------------------------------------------------------------

# --- æ›´æ”¹: æ·»åŠ äº† 'all_scores' å‚æ•° ---
def process_and_evaluate_file(filepath: str, all_scores: Dict[int, List[float]]):
    """
    ä» .jsonl æ–‡ä»¶è¯»å–æ•°æ®ï¼Œä½¿ç”¨é¢„åŠ è½½çš„åˆ†æ•°ç”Ÿæˆæ’åï¼Œå¹¶è°ƒç”¨è¯„ä¼°å‡½æ•°ã€‚
    """
    print(f"æ­£åœ¨å¼€å§‹åˆ†ææ–‡ä»¶: {filepath}...")
    
    results_for_evaluation: List[Dict] = [] # ç”¨äºå­˜å‚¨æ‰€æœ‰æ’åçš„åˆ—è¡¨
    total_lines = 0
    error_lines = 0
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            # --- 'i' æ˜¯ 0-based è¡Œç´¢å¼• ---
            for i, line in enumerate(f):
                line_num = i + 1
                total_lines += 1
                
                try:
                    data = json.loads(line)

                    # 1. æå– 'answers' åˆ—è¡¨
                    if 'answers' not in data or not isinstance(data['answers'], list):
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'answers' é”®ç¼ºå¤±æˆ–ä¸æ˜¯åˆ—è¡¨ã€‚")
                        error_lines += 1
                        continue
                    
                    answers_data = data['answers']
                    
                    # 2. æå– 'verify_result' (1-based æ’å) åˆ—è¡¨
                    if 'verify_result' not in data or not isinstance(data['verify_result'], list):
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' é”®ç¼ºå¤±æˆ–ä¸æ˜¯åˆ—è¡¨ã€‚")
                        error_lines += 1
                        continue
                    
                    verified_1_based_ranking = data['verify_result'] 

                    # 3. (æ›´æ”¹) ä¸¥æ ¼æ£€æŸ¥æ¯è¡Œæ˜¯å¦æ°å¥½æœ‰ 3 ä¸ªå›ç­”
                    if len(answers_data) != 3 or len(verified_1_based_ranking) != 3:
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: è¯¥è¡Œä¸åŒ…å« 3 ä¸ªå›ç­”ã€‚ 'answers' é•¿åº¦: {len(answers_data)}, 'verify_result' é•¿åº¦: {len(verified_1_based_ranking)}")
                        error_lines += 1
                        continue

                    # 4. (æ›´æ”¹) ç”Ÿæˆæ’å
                    
                    # (A) é¢„æµ‹æ’å: 
                    # 
                    # --- æ›´æ”¹å¼€å§‹: ä»é¢„åŠ è½½çš„åˆ—è¡¨ 'all_scores' ä¸­è·å–åˆ†æ•° ---
                    raw_prob_list = []
                    try:
                        # 'i' æ˜¯ 0-based è¡Œç´¢å¼•
                        score_pos_0 = all_scores[0][i] # 1.json[i]
                        score_pos_1 = all_scores[1][i] # 2.json[i]
                        score_pos_2 = all_scores[2][i] # 3.json[i]
                        raw_prob_list = [score_pos_0, score_pos_1, score_pos_2]
                    
                    except IndexError:
                        # å½“ .jsonl æ–‡ä»¶è¡Œæ•° > åˆ†æ•°åˆ—è¡¨é•¿åº¦æ—¶è§¦å‘
                        logging.error(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: ç´¢å¼• {i} è¶…å‡ºäº†åˆ†æ•°åˆ—è¡¨çš„èŒƒå›´ (é•¿åº¦ {len(all_scores[0])})ã€‚")
                        logging.error("è¯·æ£€æŸ¥æ‚¨çš„ .jsonl æ–‡ä»¶å’Œ 1.json/2.json/3.json æ–‡ä»¶æ˜¯å¦åŒ¹é…ã€‚")
                        error_lines += 1
                        continue # è·³è¿‡å½“å‰ .jsonl è¡Œ
                    # --- æ›´æ”¹ç»“æŸ ---

                    
                    # (ii) ä»åˆ†æ•° -> 0-based æ’å
                    predicted_ranking_list = get_ranking_from_scores(raw_prob_list)
                    
                    # (B) çœŸå®æ’å: ä» 1-based æ’å -> 0-based æ’å
                    try:
                        # e.g., [2, 1, 3] -> [1, 0, 2]
                        verified_0_based_ranking = [idx - 1 for idx in verified_1_based_ranking]
                        
                        # éªŒè¯è½¬æ¢åçš„ç´¢å¼•
                        num_items = len(verified_0_based_ranking) # åº”è¯¥æ€»æ˜¯ 3
                        if not all(0 <= idx < num_items for idx in verified_0_based_ranking):
                            logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' åŒ…å«æ— æ•ˆçš„ 1-based ç´¢å¼• (ä¾‹å¦‚ 0 æˆ– å¤§äº {num_items})ã€‚")
                            error_lines += 1
                            continue
                        if len(set(verified_0_based_ranking)) != num_items:
                            logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' è½¬æ¢ååŒ…å«é‡å¤çš„ 0-based ç´¢å¼•ã€‚")
                            error_lines += 1
                            continue
                    
                    except TypeError:
                        logging.warning(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: 'verify_result' åŒ…å«éæ•´æ•°é¡¹ã€‚")
                        error_lines += 1
                        continue
                    
                    # 5. å°†æ’ååˆ—è¡¨æ·»åŠ åˆ°æˆ‘ä»¬çš„ç»“æœé›†ä¸­
                    results_for_evaluation.append({
                        "verify_result": verified_0_based_ranking,    # æ ¼å¼: [1, 0, 2]
                        "predicted_ranking": predicted_ranking_list   # æ ¼å¼: [1, 0, 2]
                    })

                except json.JSONDecodeError:
                    logging.error(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: JSON è§£æé”™è¯¯ã€‚")
                    error_lines += 1
                except (KeyError, TypeError, AttributeError, ValueError) as e:
                    logging.error(f"è·³è¿‡ç¬¬ {line_num} è¡Œ: å¤„ç†æ•°æ®æ—¶å‡ºé”™ - {e}")
                    error_lines += 1

    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ '{filepath}' æœªæ‰¾åˆ°ã€‚", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)

    # --- æ–‡ä»¶å¤„ç†æ‘˜è¦ ---
    print("\n--- ğŸ“ æ–‡ä»¶å¤„ç†æ‘˜è¦ ---")
    print(f"æ€»å…±æ£€æŸ¥è¡Œæ•°: {total_lines}")
    print(f"è·³è¿‡/é”™è¯¯è¡Œæ•°: {error_lines}")
    valid_lines = total_lines - error_lines
    print(f"æœ‰æ•ˆå‚ä¸è¯„ä¼°è¡Œæ•°: {valid_lines}")

    if valid_lines > 0:
        # 6. (å¾ªç¯ç»“æŸå) è°ƒç”¨è¯„ä¼°å‡½æ•°
        print("æ­£åœ¨è®¡ç®—æœ€ç»ˆæ’åç»Ÿè®¡æ•°æ®...")
        evaluate_final_results_ranking(results_for_evaluation)
    else:
        print("æ²¡æœ‰å¯ç”¨äºè¯„ä¼°çš„æœ‰æ•ˆæ•°æ®è¡Œã€‚")


# --- è„šæœ¬ä¸»å…¥å£ ---
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—è®°å½•
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s') 
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python rank_and_evaluate.py <your_file.jsonl>")
        print("ç¤ºä¾‹: python rank_and_evaluate.py data.jsonl")
        sys.exit(1)
        
    file_path = sys.argv[1]
    
    # 1. (æ›´æ”¹) é¢„åŠ è½½æ‰€æœ‰åˆ†æ•°
    # æ£€æŸ¥ SCORE_FILES_DIR æ˜¯å¦å­˜åœ¨
    if not os.path.isdir(SCORE_FILES_DIR):
        logging.error(f"é”™è¯¯: é…ç½®çš„åˆ†æ•°ç›®å½• 'SCORE_FILES_DIR' ä¸å­˜åœ¨: {SCORE_FILES_DIR}")
        logging.error("è¯·åœ¨è„šæœ¬é¡¶éƒ¨åˆ›å»ºæ­¤ç›®å½•æˆ–ä¿®æ”¹ 'SCORE_FILES_DIR' å˜é‡ã€‚")
        sys.exit(1)
    
    all_scores = load_all_scores(SCORE_FILES_DIR)
    
    # 2. (æ›´æ”¹) å¥å…¨æ€§æ£€æŸ¥ï¼šæ¯”è¾ƒ .jsonl è¡Œæ•°å’Œåˆ†æ•°åˆ—è¡¨é•¿åº¦
    try:
        score_list_length = len(all_scores[0])
        jsonl_line_count = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            for _ in f:
                jsonl_line_count += 1
        
        if jsonl_line_count != score_list_length:
            logging.warning(f"!! è­¦å‘Š: æ–‡ä»¶ '{file_path}' æœ‰ {jsonl_line_count} è¡Œ,")
            logging.warning(f"   ä½†åˆ†æ•°æ–‡ä»¶ (1.json ç­‰) åŒ…å« {score_list_length} ä¸ªåˆ†æ•°ã€‚")
            logging.warning("   å°†ç»§ç»­å¤„ç†ï¼Œä½†å¦‚æœè¡Œæ•°ä¸åŒ¹é…ï¼Œè¯„ä¼°å¯èƒ½ä¸å‡†ç¡®æˆ–åœ¨ä¸­é€”å¤±è´¥ã€‚")
        else:
            logging.info(f"âœ… æ–‡ä»¶è¡Œæ•° ({jsonl_line_count}) ä¸åˆ†æ•°åˆ—è¡¨é•¿åº¦ ({score_list_length}) åŒ¹é…ã€‚")

    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"æ£€æŸ¥æ–‡ä»¶è¡Œæ•°æ—¶å‡ºé”™: {e}", file=sys.stderr)
        sys.exit(1)

    
    # 3. (æ›´æ”¹) å¤„ç†æ–‡ä»¶å¹¶è¯„ä¼°
    process_and_evaluate_file(file_path, all_scores)